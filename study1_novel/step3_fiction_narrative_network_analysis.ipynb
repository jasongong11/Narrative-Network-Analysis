{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3da92d62-44f7-49ec-9043-108c20239b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f720e9-bc0d-480a-bfa0-ed95fbbe9701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: NX_CUGRAPH_AUTOCONFIG=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xjgong/anaconda3/envs/narrative/lib/python3.11/site-packages/networkx/utils/backends.py:119: RuntimeWarning: Error encountered when loading info for backend cugraph: No module named '_nx_cugraph'\n",
      "  backend_info.update(_get_backends(\"networkx.backend_info\", load_and_call=True))\n"
     ]
    }
   ],
   "source": [
    "%env NX_CUGRAPH_AUTOCONFIG=True\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "441d82a5-60a3-47d2-8ac6-cd619baf98aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(filename):\n",
    "    with open(filename) as file:\n",
    "        data=json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd2873dd-e2d7-4b46-91d0-cdaba4b36ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_character_data(data, printTop):\n",
    "    character_data = []\n",
    "    \n",
    "    for character in data[\"characters\"]:\n",
    "    \n",
    "        agentList=\" \".join([i[\"w\"].lower() for i in character[\"agent\"]])\n",
    "        patientList=\" \".join([i[\"w\"].lower() for i in character[\"patient\"]])\n",
    "        modList=\" \".join([i[\"w\"] for i in character[\"mod\"] if len(character[\"mod\"]) > 0])\n",
    "    \n",
    "        character_id=character[\"id\"]\n",
    "        count=character[\"count\"]\n",
    "    \n",
    "        referential_gender_distribution=referential_gender_prediction=\"unknown\"\n",
    "    \n",
    "        if character[\"g\"] is not None and character[\"g\"] != \"unknown\":\n",
    "            referential_gender_distribution=character[\"g\"][\"inference\"]\n",
    "            referential_gender=character[\"g\"][\"argmax\"]\n",
    "    \n",
    "        mentions=character[\"mentions\"]\n",
    "        proper_mentions=mentions[\"proper\"]\n",
    "        max_proper_mention=\"\"\n",
    "        role = \"\"\n",
    "        if len(proper_mentions) > 0:\n",
    "            max_proper_mention = mentions[\"proper\"][0][\"n\"]\n",
    "            role = \"proper\"\n",
    "        elif len(mentions[\"common\"]) > 0:\n",
    "            max_proper_mention = mentions[\"common\"][0][\"n\"]\n",
    "            role = \"common\"\n",
    "        elif len(mentions[\"pronoun\"]) > 0 and mentions[\"pronoun\"][0][\"n\"] in [\"I\", \"My\", \"my\", \"mine\"]:\n",
    "            max_proper_mention = \"I\"\n",
    "            role = \"self\"\n",
    "            \n",
    "        temp={\"id\": character[\"id\"],\n",
    "              \"name\": max_proper_mention,\n",
    "              \"gender\": referential_gender,\n",
    "              \"mod\": modList,\n",
    "              \"agent_list\": agentList,\n",
    "              \"patient_list\": patientList,\n",
    "              \"count\": count,\n",
    "              \"role\": role}    \n",
    "    \n",
    "        character_data.append(temp)\n",
    "    \n",
    "    character_data = pd.DataFrame(character_data)\n",
    "    character_data = character_data[character_data[\"count\"] >= printTop]\n",
    "    return character_data\n",
    "    \n",
    "def get_event_character_df(book, folder, count_threshold = 1):\n",
    "    \n",
    "    book_file = \"{}/{}/{}.book\".format(folder,book,book)\n",
    "    data = proc(book_file)\n",
    "    \n",
    "    token_file = \"{}/{}/{}.tokens\".format(folder,book,book)\n",
    "    token_df = pd.read_csv(token_file, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, encoding='utf-8')\n",
    "    events_df = token_df.copy()\n",
    "\n",
    "    # Processing patient data\n",
    "    patient_df = []\n",
    "    for cha in data[\"characters\"]:\n",
    "        for i in cha[\"patient\"]:\n",
    "            cha_patient_temp = {\"patient\": cha[\"id\"],\n",
    "                                \"event\": i[\"w\"],\n",
    "                                \"event_id\": i[\"i\"]}\n",
    "            patient_df.append(cha_patient_temp)\n",
    "    patient_df = pd.DataFrame(patient_df)\n",
    "    \n",
    "    # Processing agent data\n",
    "    agent_df = []\n",
    "    for cha in data[\"characters\"]:\n",
    "        for i in cha[\"agent\"]:\n",
    "            cha_agent_temp = {\"agent\": cha[\"id\"],\n",
    "                                \"event\": i[\"w\"],\n",
    "                                \"event_id\": i[\"i\"]}\n",
    "            agent_df.append(cha_agent_temp)\n",
    "    agent_df = pd.DataFrame(agent_df)\n",
    "\n",
    "    # Combining with events data\n",
    "    events_df = events_df.merge(patient_df[[\"event_id\", \"patient\"]], how=\"left\", left_on=\"token_ID_within_document\", right_on=\"event_id\")\n",
    "    events_df = events_df.merge(agent_df[[\"event_id\", \"agent\"]], how=\"left\", left_on=\"token_ID_within_document\", right_on=\"event_id\")\n",
    "    events_df = events_df[~events_df[[\"agent\", \"patient\"]].isnull().any(axis=1)]\n",
    "    events_df = events_df.drop([\"event_id_x\", \"event_id_y\"], axis=1)\n",
    "    events_df = events_df[~(events_df.agent == events_df.patient)] # filter out events that agent and patient are the same\n",
    "\n",
    "    # extract characters\n",
    "    character_data = create_character_data(data, count_threshold)\n",
    "    character_data = character_data[~(character_data.name=='')] # filter out characters that do not have a name\n",
    "\n",
    "    # Combine events with characters\n",
    "    events_df = events_df.merge(character_data[[\"id\", \"name\"]].add_prefix('agent_'), how=\"left\", left_on=\"agent\", right_on=\"agent_id\")\n",
    "    events_df = events_df.merge(character_data[[\"id\", \"name\"]].add_prefix('patient_'), how=\"left\", left_on=\"patient\", right_on=\"patient_id\")\n",
    "    events_df = events_df.dropna(subset=[\"agent_name\", \"patient_name\"])\n",
    "    \n",
    "    character_data_new = character_data.drop_duplicates(subset=[\"name\"]).reset_index(drop=True)\n",
    "    character_data_new[\"mod\"] = character_data_new[\"name\"].map(character_data.groupby(\"name\")[\"mod\"].apply(list).apply(lambda x: \" \".join(x)))\n",
    "    character_data_new[\"agent_list\"] = character_data_new[\"name\"].map(character_data.groupby(\"name\")[\"agent_list\"].apply(list).apply(lambda x: \" \".join(x)))\n",
    "    character_data_new[\"patient_list\"] = character_data_new[\"name\"].map(character_data.groupby(\"name\")[\"patient_list\"].apply(list).apply(lambda x: \" \".join(x)))\n",
    "    events_df = events_df[~(events_df.agent_name==events_df.patient_name)]\n",
    "\n",
    "    return character_data_new, events_df, token_df\n",
    "\n",
    "def get_network_metrics(G0):\n",
    "    \n",
    "    n_nodes = G0.number_of_nodes()\n",
    "    n_edges = G0.number_of_edges()\n",
    "    network_density = nx.density(G0)\n",
    "    \n",
    "    if n_nodes > 1 and n_edges > 1:\n",
    "\n",
    "        # Average Clustering\n",
    "        average_clustering = nx.average_clustering(G0, weight=\"weight\")\n",
    "\n",
    "        # Modularity\n",
    "        communities = nx.community.louvain_communities(G0, weight=\"weight\")\n",
    "        modularity = nx.community.modularity(G0,communities, weight=\"weight\" )\n",
    "\n",
    "        # Shortest Path Length\n",
    "        average_shortest_path_largest_component = nx.average_shortest_path_length(G0, weight=None)\n",
    "            \n",
    "\n",
    "        network_metrics = {\"n_nodes\": n_nodes,\n",
    "                           \"n_edges\": n_edges,\n",
    "                           \"network_density\": network_density,\n",
    "                           \"average_clustering\": average_clustering,\n",
    "                           \"modularity\": modularity,\n",
    "                           \"average_shortest_path_largest_component\": average_shortest_path_largest_component}\n",
    "\n",
    "    else:\n",
    "        network_metrics = {\"n_nodes\": n_nodes,\n",
    "                           \"n_edges\": n_edges,\n",
    "                           \"network_density\": np.nan,\n",
    "                           \"average_clustering\": np.nan,\n",
    "                           \"modularity\": np.nan,\n",
    "                           \"average_shortest_path_largest_component\": np.nan}\n",
    "    return network_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92099d7c-d214-4d33-988a-346348ebfa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = pd.read_csv(\"./gutenberg_meta_df.csv\")\n",
    "books = meta_df.id.unique()\n",
    "\n",
    "books_list = []\n",
    "for b in tqdm(books):\n",
    "    outputDir = \"../../gutenberg_standard/new_analysis/book_nlp_output_small/{}/\".format(b)\n",
    "    idd = b\n",
    "    if os.path.exists(outputDir + \"{}.book\".format(b)):\n",
    "        books_list.append(b)\n",
    "\n",
    "meta_df = meta_df[meta_df.id.isin(books_list)]\n",
    "meta_df=meta_df.drop_duplicates(subset=[\"id\"])\n",
    "\n",
    "books = meta_df.id.unique()\n",
    "print(len(books))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f488d9e-2d51-4542-9553-0e46867a0148",
   "metadata": {},
   "source": [
    "# Load Events Data and Character Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41af62cf-b2af-4b24-b730-a9c86014ec9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████████████████████████████████████████████████████████████████████████████████████████████▍                                | 8112/10879 [21:12<04:28, 10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot access local variable 'referential_gender' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|██████████████████████████████████████████████████████████████████████████████████████████████████                              | 8333/10879 [21:54<04:49,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot access local variable 'referential_gender' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋                     | 9069/10879 [23:49<03:56,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot access local variable 'referential_gender' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10879/10879 [29:16<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "events_df_dict = {}\n",
    "characters_df_dict = {}\n",
    "token_df_df_dict = {}\n",
    "for book in tqdm(books):\n",
    "    try:\n",
    "        character_data, events_df, token_df = get_event_character_df(book, \"../../gutenberg_standard/new_analysis/book_nlp_output_small/\", count_threshold=1)\n",
    "        events_df_dict[book] = events_df\n",
    "        characters_df_dict[book] = character_data\n",
    "        token_df_df_dict[book] = token_df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6361c0c8-2757-45a9-ae6a-98d452c222f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10876/10876 [00:00<00:00, 12255.95it/s]\n"
     ]
    }
   ],
   "source": [
    "N_tokens_dict = {}\n",
    "N_sentences_dict = {}\n",
    "for book in tqdm(token_df_df_dict.keys()):\n",
    "    token_df = token_df_df_dict[book]\n",
    "    N_tokens = token_df.token_ID_within_document.max() + 1\n",
    "    N_sentences = token_df.sentence_ID.max() + 1\n",
    "    N_tokens_dict[book] = N_tokens\n",
    "    N_sentences_dict[book] = N_sentences\n",
    "\n",
    "meta_df[\"N_tokens\"] = meta_df.id.map(N_tokens_dict)\n",
    "meta_df[\"log_N_tokens\"] = np.log(meta_df[\"N_tokens\"])\n",
    "meta_df[\"N_sentences\"] = meta_df.id.map(N_sentences_dict)\n",
    "meta_df[\"log_N_sentences\"] = np.log(meta_df[\"N_sentences\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c9a85-c369-4cfe-b2f9-87a16fbb1180",
   "metadata": {},
   "source": [
    "# Narrative Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "232ac2b2-3f97-45bb-a3a7-e93925e1654d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10876/10876 [00:22<00:00, 486.28it/s]\n"
     ]
    }
   ],
   "source": [
    "networks_dict = {}\n",
    "for book in tqdm(events_df_dict.keys()):\n",
    "    \n",
    "    events_df = events_df_dict[book]\n",
    "    character_data = characters_df_dict[book]\n",
    "\n",
    "    character_data = character_data[character_data.role == \"proper\"]\n",
    "    events_df = events_df[(events_df.agent_name.isin(character_data.name.unique()))&(events_df.patient_name.isin(character_data.name.unique()))]\n",
    "    events_df = events_df[~(events_df.agent_name==events_df.patient_name)]\n",
    "\n",
    "    edge_list = events_df[[\"agent_name\", \"patient_name\", \"lemma\"]]\n",
    "    \n",
    "    if edge_list.shape[0] >= 10:\n",
    "        G_multi = nx.from_pandas_edgelist(edge_list, source=\"agent_name\", target=\"patient_name\", edge_attr=[\"lemma\"], create_using=nx.MultiGraph).to_undirected()\n",
    "        edge_counts = Counter((min(u, v), max(u, v)) for u, v in G_multi.edges())\n",
    "        G = nx.Graph()\n",
    "        G.add_edges_from((u, v, {'weight': w}) for (u, v), w in edge_counts.items())\n",
    "        Gcc = sorted(nx.connected_components(G.to_undirected()), key=len, reverse=True)\n",
    "        G0 = G.to_undirected().subgraph(Gcc[0])\n",
    "        \n",
    "        networks_dict[book] = G0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf9f300-6432-42c1-837a-f3aa762ea8e6",
   "metadata": {},
   "source": [
    "# Network Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "661e1253-6454-4ffb-835e-16dba08886de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10605/10605 [00:57<00:00, 184.01it/s]\n"
     ]
    }
   ],
   "source": [
    "network_metrics_dict = {}\n",
    "for book in tqdm(networks_dict.keys()):\n",
    "    G0 = networks_dict[book]\n",
    "    if G0.number_of_nodes() > 2:\n",
    "        network_metrics = get_network_metrics(networks_dict[book])\n",
    "        network_metrics_dict[book] = network_metrics\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b86b1ad-36bd-4c74-8f1a-81d8ca6bac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_metric_df = pd.DataFrame(network_metrics_dict).T.reset_index()\n",
    "meta_df = meta_df.merge(network_metric_df.reset_index(), how=\"left\", left_on=\"id\", right_on=\"index\")\n",
    "meta_df.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0b15537-2db0-4b47-9614-16b6435853b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_variables = [\"n_nodes\", \"network_density\", \"modularity\", \"average_clustering\", \"average_shortest_path_largest_component\"]\n",
    "filter = ~meta_df[network_variables].isna().any(axis=1)\n",
    "meta_df = meta_df[filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d7c8430-f9ee-46ab-9d0f-c3cbdc031f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[[\"id\", \"title\", \"author\", \"authoryearofbirth\", \"authoryearofdeath\", \"language\",\n",
    "        \"downloads\", 'genre_war', 'genre_biography', 'genre_romance',\n",
    "       'genre_drama', 'genre_fantasy', 'genre_family', 'genre_science',\n",
    "       'genre_action', 'genre_thriller', 'genre_western', 'genre_horror',\n",
    "       'genre_mystery', 'genre_crime', 'genre_history', 'genre_periodicals',\n",
    "       'genre_christian', 'genre_other', 'N_tokens',\n",
    "       'log_N_tokens', 'N_sentences', 'log_N_sentences', \n",
    "         'n_nodes', 'n_edges', 'network_density', 'average_clustering',\n",
    "       'modularity', 'average_shortest_path_largest_component']].to_csv(\"../data/study1_novel/fiction_regression_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d86325-5c4e-4497-b20b-e7772e675022",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
